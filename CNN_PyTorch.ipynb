{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão Python: 3.9.7\n"
     ]
    }
   ],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão Python:', python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neurais Convolucionais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Rodrigo Abreu\n",
      "\n",
      "numpy      : 1.19.5\n",
      "torch      : 1.10.1\n",
      "torchvision: 0.11.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Rodrigo Abreu\" --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe Para o Modelo\n",
    "class ModeloCNN(nn.Module):\n",
    "    \n",
    "    # Construtor da Classe\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Camada de Entrada\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding = 1)\n",
    "        \n",
    "        # Primeira Camada Oculta\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding = 1)\n",
    "        \n",
    "        # Segunda Camada Oculta\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding = 1)\n",
    "        \n",
    "        # Camada de Pooling\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Transformação Linear 1 (Camada Densa)\n",
    "        self.linear1 = nn.Linear(64 * 4 * 4, 512)\n",
    "        \n",
    "        # Transformação Linear 2 (Camada Densa)\n",
    "        self.linear2 = nn.Linear(512, 10) \n",
    "        \n",
    "        # Camada de Dropout Para Regularização\n",
    "        self.dropout = nn.Dropout(p = 0.3)\n",
    "    \n",
    "    # Método de Avanço da Rede\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando as Transformações Para os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformações Para os Dados de Entrada\n",
    "transformations = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando e Dividindo os Dados em Treino, Teste e Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to CIFAR10\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f9300083aa442f94fce0c391395ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting CIFAR10\\cifar-10-python.tar.gz to CIFAR10\n"
     ]
    }
   ],
   "source": [
    "# Carrega os Dados de Treino\n",
    "train_data = datasets.CIFAR10('CIFAR10', train = True, download = True, transform = transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Carrega os Dados de Teste\n",
    "test_data = datasets.CIFAR10('CIFAR10', train = False, download = True, transform = transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tamanho dos Datasets\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamanho Para Dados de Validação\n",
    "validation_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamanho Para Dados de Treino\n",
    "training_size = len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Índice \n",
    "indices = list(range(training_size))\n",
    "np.random.shuffle(indices)\n",
    "index_split = int(np.floor(training_size * validation_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Índices Para Dados de Treino e Validação\n",
    "validation_indices, training_indices = indices[:index_split], indices[index_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amostras de Treino e Validação\n",
    "training_sample = SubsetRandomSampler(training_indices)\n",
    "validation_sample = SubsetRandomSampler(validation_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparâmetros\n",
    "batch_size = 16\n",
    "n_epochs = 30\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera as Amostras Finais de Dados\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, sampler = training_sample)\n",
    "valid_loader = DataLoader(train_data, batch_size = batch_size, sampler = validation_sample)\n",
    "test_loader = DataLoader(train_data, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otimização do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o Modelo (Instância da Classe)\n",
    "model = ModeloCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModeloCNN(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (linear2): Linear(in_features=512, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identifica o Dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Envia o Modelo para a GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Função de Custo Baseada em Cross Entropia\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o Otimizador (Backward)\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função Para Acurácia\n",
    "def accuracy(preds, y):\n",
    "    _, pred = torch.argmax(preds, 1)\n",
    "    correct = pred.eq(target.data.view_as(pred))\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Erro em Treino: 2.037 | Erro em Validação: 1.734 |\n",
      "| Epoch: 02 | Erro em Treino: 1.668 | Erro em Validação: 1.515 |\n",
      "| Epoch: 03 | Erro em Treino: 1.508 | Erro em Validação: 1.403 |\n",
      "| Epoch: 04 | Erro em Treino: 1.412 | Erro em Validação: 1.327 |\n",
      "| Epoch: 05 | Erro em Treino: 1.343 | Erro em Validação: 1.268 |\n",
      "| Epoch: 06 | Erro em Treino: 1.280 | Erro em Validação: 1.219 |\n",
      "| Epoch: 07 | Erro em Treino: 1.228 | Erro em Validação: 1.166 |\n",
      "| Epoch: 08 | Erro em Treino: 1.177 | Erro em Validação: 1.101 |\n",
      "| Epoch: 09 | Erro em Treino: 1.144 | Erro em Validação: 1.076 |\n",
      "| Epoch: 10 | Erro em Treino: 1.099 | Erro em Validação: 1.033 |\n",
      "| Epoch: 11 | Erro em Treino: 1.065 | Erro em Validação: 1.000 |\n",
      "| Epoch: 12 | Erro em Treino: 1.035 | Erro em Validação: 0.976 |\n",
      "| Epoch: 13 | Erro em Treino: 1.007 | Erro em Validação: 0.967 |\n",
      "| Epoch: 14 | Erro em Treino: 0.982 | Erro em Validação: 0.933 |\n",
      "| Epoch: 15 | Erro em Treino: 0.959 | Erro em Validação: 0.915 |\n",
      "| Epoch: 16 | Erro em Treino: 0.939 | Erro em Validação: 0.870 |\n",
      "| Epoch: 17 | Erro em Treino: 0.919 | Erro em Validação: 0.862 |\n",
      "| Epoch: 18 | Erro em Treino: 0.903 | Erro em Validação: 0.878 |\n",
      "| Epoch: 19 | Erro em Treino: 0.884 | Erro em Validação: 0.861 |\n",
      "| Epoch: 20 | Erro em Treino: 0.873 | Erro em Validação: 0.836 |\n",
      "| Epoch: 21 | Erro em Treino: 0.861 | Erro em Validação: 0.829 |\n",
      "| Epoch: 22 | Erro em Treino: 0.844 | Erro em Validação: 0.821 |\n",
      "| Epoch: 23 | Erro em Treino: 0.835 | Erro em Validação: 0.799 |\n",
      "| Epoch: 24 | Erro em Treino: 0.819 | Erro em Validação: 0.785 |\n",
      "| Epoch: 25 | Erro em Treino: 0.808 | Erro em Validação: 0.794 |\n",
      "| Epoch: 26 | Erro em Treino: 0.801 | Erro em Validação: 0.791 |\n",
      "| Epoch: 27 | Erro em Treino: 0.789 | Erro em Validação: 0.770 |\n",
      "| Epoch: 28 | Erro em Treino: 0.781 | Erro em Validação: 0.787 |\n",
      "| Epoch: 29 | Erro em Treino: 0.770 | Erro em Validação: 0.767 |\n",
      "| Epoch: 30 | Erro em Treino: 0.764 | Erro em Validação: 0.762 |\n"
     ]
    }
   ],
   "source": [
    "# Looop\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    \n",
    "    # Inicializa o Erro a Cada Epoch\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # Cria Objeto de Treinamento do Modelo\n",
    "    model.train()\n",
    "    \n",
    "    # Para Cada Batch de Dados Executa o Treinamento\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # Cria Objeto de Avaliação do Modelo    \n",
    "    model.eval()\n",
    "    \n",
    "    # Para Cada Batch de Dados Executa a Avaliiação\n",
    "    for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # Calcula o Erro na Epoch\n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "    \n",
    "    # Print\n",
    "    print(f'| Epoch: {epoch:02} | Erro em Treino: {train_loss:.3f} | Erro em Validação: {valid_loss:.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva o modelo treinado\n",
    "torch.save(model.state_dict(), \"modelos/cifar10.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 10000\n",
       "    Root location: CIFAR10\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               RandomHorizontalFlip(p=0.5)\n",
       "               RandomRotation(degrees=[-20.0, 20.0], interpolation=nearest, expand=False, fill=0)\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "           )"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
